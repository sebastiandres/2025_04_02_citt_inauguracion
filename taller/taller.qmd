---
execute:
  echo: true
format:
  revealjs:
    width: 1245
    height: 700
    menu: false
    controls: true
    transition: fade
    auto-stretch: false
    embed-resources: false
    toc: false
    center: true
    slide-number: false
    preview-links: false
    output-file: "taller"
    theme:
        - simple
        - style/style_background.scss

---

##

```
        - style/style_background.scss
```

---

## { .custom-slide }
::: {style="position: absolute; left: 700px; top: 550px; width:2000px; background-color: #ffffff; padding: 10px; border-radius: 5px;"}
[Taller de Python + IA para todos]{style="font-size: 20px; margin: 0px;"} <br>
[¬°Haz tu propio ChatGPT!]{style="font-size: 30px; font-weight: bold; margin: 0px"} <br>
[Sebasti√°n Flores, Francisco Alfaro, & Valeska Canales]{style="font-size: 25px;"}
:::

---

## Programa del taller

::: {.incremental}
* Algunos conceptos te√≥ricos: Tokens, LLMs, y APIs.
* 5 actividades pr√°cticas
:::

---

## ¬øQu√© dicen los diarios de la IA?

![](images/alarmismo.png){fig-align="center" .fragment}

---

## [SI]{style="color: red;"} presten atenci√≥n al hombre tras la cortina

{{< video videos/tras_la_cortina.mp4 height="500px" >}}

---

::: {.callout-tip title="Recuerden:"}
Toda tecnolog√≠a suficientemente avanzada parece magia. *Arthur C. Clarke*
:::

---

## Parte 1 {.custom-section}

La ilusi√≥n de la continuidad

---

## Desaf√≠o

¬øQu√© contiene la siguiente cadena de bits?

[00000000]{style="font-size: 80px; margin: 0px; color: white"}
[00101010]{style="font-size: 80px; margin: 0px;"}

---

[00000000]{style="font-size: 80px; margin: 0px; color: white"}
[00101010]{style="font-size: 80px; margin: 0px;"}

[Podr√≠a ser el n√∫mero 42 escrito en binario...]{.fragment}

---

[00000000]{style="font-size: 80px; margin: 0px; color: white"}
[00101010]{style="font-size: 80px; margin: 0px;"}

[Podr√≠a ser el car√°cter `*` en la convenci√≥n ascii...]{.fragment}

---

[00000000]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00101010]{style="font-size: 80px; margin: 0px;"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}

[Podr√≠a ser parte de un n√∫mero decimal, 0.4523 o $\pi$...]{.fragment}


---

[10000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00100001]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[01000111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00001000]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00101010]{style="font-size: 80px; margin: 0px;"}
[01000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[11111111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[11001101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[01000111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[01110111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}

[Podr√≠a ser parte de un archivo multimedia (video, imagen, audio, etc.)...]{.fragment}

---

En el computador **TODO** se representa con bits.

`representaci√≥n` `=` `bits` `+` `contexto` 

[Eso significa que todo es discreto.  
No existe ni el infinito ni lo continuo.]{.fragment}

---

Alta fidelidad no es continuidad.

![](code/derivada_5.png){fig-align="center"}

---

Alta fidelidad no es continuidad.

![](code/derivada_7.png){fig-align="center"}

---

Alta fidelidad no es continuidad.

![](code/derivada_9.png){fig-align="center"}

---

Alta fidelidad no es continuidad.

![](code/derivada_11.png){fig-align="center"}

---

Alta fidelidad no es continuidad.

![](code/derivada_21.png){fig-align="center"}

---

Alta fidelidad no es continuidad. [Pero puede ser suficiente...]{.fragment fragment-index=1}

![](code/derivada_31.png){fig-align="center"}

---

No necesitamos la realidad, necesitamos una buena aproximaci√≥n. Suficiente para enga√±ar a los sentidos.

![](images/celuloide.jpg){width=50% fig-align="center" .fragment}

[Una pel√≠cula de 24 FPS es suficiente para enga√±ar al ojo humano.]{.fragment}


---

## ¬øQu√© aprendimos?

::: {.incremental}
* Ninguna representaci√≥n en el computador es perfecta.
* LLMs no son perfectos, pero ya son suficientemente √∫tiles.
:::

---

## Preguntas {.questions-slide}

---

## Parte 2 {.custom-section}

El computador parlanch√≠n

---

## ¬øC√≥mo representar una palabra?  

::: {.incremental}
* Representaci√≥n textual
* Representaci√≥n sem√°ntica
:::

---

## Representaci√≥n textual

Si solo queremos transcribir texto, basta con representar cada letra con una secuencia de bits, y almacenarla.

::: columns
::: {.column style="font-size: 24px;" .fragment .center}
ASCII

* 1 byte (8 bits): 128 car√°cteres posibles
* 0 (48) ... 9 (57)
* A (65) ... Z (90)
* a (97) ... z (122) 
* Problema: Faltan muchos car√°cteres: √ë, √±, √°, √©, √≠, √≥, √∫, u
:::
::: {.column style="font-size: 24px;" .fragment .center}
UTF-8

* 1 a 4 bytes (8 a 32 bits)
* Mantiene ASCII sin cambios
* Permite representar alfabetos latinos, griego, cir√≠lico, copto, armenio, hebreo, √°rabe, sir√≠aco, thaana, y n'ko, adem√°s de caracteres chinos, japoneses y coreanos. 
* Incluye emojis üòÅ, simbolos ‚úÖ y mil cosas m√°s üóø
:::
:::

---

## Representaci√≥n sem√°ntica

Sem√°ntica: relativo al **significado** de las palabras.

Si quieres que el computador pueda interpretar el sentido de cada palabra, es necesario almacenar cada palabra como un todo. No puede descomponerse en sus letras.

[Necesitamos una mejor REPRESENTACI√ìN.]{.fragment}

[La palabra se guarda entera o se descompone en sus partes representativas ([tokens]{style="color: red;"})]{.fragment}

---

## Actividad 2.1

* **Actividad**: Ir a [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
* **Objetivo**: Evaluar distintos textos, en distintos idiomas.
  * Espa√±ol: [*La inform√°tica,‚Äã tambi√©n llamada computaci√≥n, es el √°rea de la ciencia que se encarga de estudiar la administraci√≥n de m√©todos, t√©cnicas y procesos con el fin de almacenar, procesar y transmitir informaci√≥n y datos en formato digital.*]{style="font-size: 24px;"}
  * Ingl√©s: [*Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes the study and experimentation of algorithmic processes, and the development of both hardware and software. Computing has scientific, engineering, mathematical, technological, and social aspects.*]{style="font-size: 24px;"}
* **Tiempo**: 5 minutos

[Nota: Definiciones extra√≠das de wikipedia.]{style="font-size: 18px;"}

---

## Aprendizajes 

::: {.incremental}
* Palabra != Token
* Cada token tiene un identificador √∫nico
* En ingl√©s,  100 tokens ~= 75 palabras. 
* Si tienen la misma representaci√≥n, tienen el mismo token (ejemplo: el papa y la papa)
:::

---

## ¬øLLM?

[LLM]{style="color: red;"} = Large Language Model = Grandes Modelos de Lenguaje

---

## Diagrama t√©cnico de un LLM

Diagrama de funcionamiento de un LLM que se filtr√≥ de OpenAI:

[**¬°¬°¬°No difundir!!!**]{style="color: red;" .fragment}

![](images/LLM_words.gif){fig-align="center" height=500px .fragment}

---

## Actividad 2.2

* **Actividad**: Ir a [https://huggingface.co/spaces/alonsosilva/NextTokenPrediction](https://huggingface.co/spaces/alonsosilva/NextTokenPrediction)
* **Objetivo**: Observar la lista de token que se muestran como posible continuaci√≥n del texto.
* **Tiempo**: 5 minutos

---

## Aprendizajes 

::: {.incremental}
* El LLM no reflexiona sobre la pr√≥xima palabra: sugiere los tokens estad√≠sticamente m√°s probables.
* Se predice token a token: es secuencial.
* El LLM no tiene memoria. Siempre empieza a predecir desde el mismo estado inicial.
:::

::: {.notes}
Comparar con una multiplicaci√≥n de matrices. La matriz no recuerda que ya hizo multiplicaciones antes.
:::

---

## El negocio de los LLMs

* Los LLMs hoy en d√≠a tienen billones de par√°metros: 1,000,000,000,000
* Cada par√°metro se determina en un proceso de entrenamiento basado en enormes conjuntos de texto (esencialmente TODO el internet y libros escritos).
* Entrenar una LLM requiere muchas horas de uso de tarjetas gr√°ficas (GPUs).

Esto significa que un LLM open source puede descargarse (son bits), y ejecutarse localmente - si tu hardware lo permite.

---

## El negocio de los LLMs

No existe solo chatGPT (OpenAI): todos quieren un pedazo de la torta:

::: columns
::: {.column width="50%" .incremental .center}
**De pago:**

* GPT-4 (OpenAI)
* Gemini (Google)
* Claude (Anthropic)
* ...
:::
::: {.column width="50%" .incremental .center}
**C√≥digo abierto:**

* Llama (Meta)
* Qwen (Baidu)
* DeepSeek (China)
* ...
:::
:::

[Y est√°n quienes no desarrollan pero entregan LLM como un servicio: Amazon (Bedrock), OpenRouter, etc.]{.fragment}

---

## Preguntas {.questions-slide}

---

## Parte 3 {.custom-section}

¬°Hazlo tu mismo!

---

## ¬øC√≥mo podemos emular chatGPT?

Lo m√°s importante es tener un LLM:

::: columns
::: {.column width="50%" .center .fragment}
Ejecutar localmente LLM: 

::: {style="font-size: 24px;"}
* Configuraci√≥n compleja
* Hardware costoso
* Hay que realizar todo el proceso: conversi√≥n de texto a tokens, predecir token a token, y conversi√≥n de tokens a texto.
:::

:::

::: {.column width="50%" .fragment}
Consumir una API de LLM  

::: {style="font-size: 24px;"}
* Simple: entrega un texto, recibe un texto.
* Pagar lo que consumes
* M√∫ltiples proveedores y alternativas 
:::

:::
:::

---

## Actividad 3.1

* **Actividad**: Ir a [https://cittripio.streamlit.app/v1](https://cittripio.streamlit.app/v1)
* **Objetivo**: Lograr que el bot responda "con personalidad"
* **Tiempo**: 5 minutos

---

## Aprendizajes 

::: {.incremental}
* El LLM responde en funci√≥n del prompt.
* El prompt puede pedir cualquier cosa.
:::

---

## Actividad 3.2

* **Actividad**: Ir a [https://cittripio.streamlit.app/v2](https://cittripio.streamlit.app/v2)
* **Objetivo**: 
  * O1: Hacer 2 preguntas relacionadas.
  * O2: Cambiar la personalidad del bot.
* **Tiempo**: 5 minutos

--- 

## Aprendizajes 

::: {.incremental}
* Separar en contexto y pregunta permite imponer una "personalidad" o ciertas caracter√≠sticas.
* Un LLM no tiene memoria.
:::

--- 

## ¬øPorqu√© chatGPT si tiene memoria? {.fragment}

¬øC√≥mo solucionar√≠an ustedes este problema?

::: {.fragment .incremental}
Pas√©mosle la historia de la conversaci√≥n en cada prompt.

- Opci√≥n 1: Pasarle todo el texto.
- Opci√≥n 2: Pasarle un resumen de la conversaci√≥n.
:::

---

##  ¬øTemperatura?

¬øQu√© es la temperatura en un LLM?

::: {.incremental}
* Es un par√°metro que controla que tan aleatoria es la elecci√≥n del siguiente token.
* Temperatura = 0: Muy determinista.
* Temperatura = 1: Muy aleatorio.
:::

---

## Actividad 3.3

* **Actividad**: Ir a [https://cittripio.streamlit.app/v3](https://cittripio.streamlit.app/v3)
* **Objetivo**: 
  * O1: Lograr que cittripio le responda a Luke Skywalker que es su padre.
  * O2: Cambiar la personalidad de cittripio por cualquier otro personaje (no necesariamente de Star Wars).
* **Tiempo**: 5 minutos

---

## Aprendizajes 

::: {.incremental}
* El LLM necesita tener como input todo el contexto e historia en el prompt.
* Las APIs agregan muchas opciones para simplificar y manejar todo esto convenientemente.
:::

---

## Conclusi√≥n {.custom-title}

::: {.incremental}
* LLMs no son magia: es tecnolog√≠a.
* Cualquiera puede comenzar a crear soluciones con LLMs.
* Conocer como funcionan LLMs permite usarlos mejor.
* Existen muchos recursos gratuitos para aprender y jugar.
:::

---

## Preguntas {.questions-slide}


![](images/qr_encuesta.png){fig-align="center" width=30%}
[https://forms.fillout.com/t/jeLVGSTdDCus](https://forms.fillout.com/t/jeLVGSTdDCus)

---

## Gracias





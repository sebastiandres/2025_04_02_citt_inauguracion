---
execute:
  echo: true
format:
  revealjs:
    width: 1245
    height: 700
    menu: false
    transition: fade
    controls: true
    auto-stretch: false
    embed-resources: true
    toc: false
    center: true
    slide-number: false
    preview-links: false
    output-file: "taller_uPlanner"
    theme:
        - simple
        - style/style_background.scss

---

## { .custom-title }
::: {style="position: absolute; left: 700px; top: 550px; width:2000px; background-color: #ffffff; padding: 10px; border-radius: 5px;"}
[Taller Pr√°ctico IA Generativa para todos]{style="font-size: 20px; margin: 0px;"} <br>
[¬°Haz tu propio ChatGPT!]{style="font-size: 30px; font-weight: bold; margin: 0px"} <br>
[Sebasti√°n Flores]{style="font-size: 25px;"}
:::

---

## Programa del taller

::: {.incremental}
* Algunos conceptos: tokens, LLMs, APIs, Gold Standard, etc.
* 5 actividades pr√°cticas
:::

---

::: {.callout-tip title="Importante:"}
Toda tecnolog√≠a suficientemente avanzada parece magia. *Arthur C. Clarke*
:::

---

## Parte 1 {.custom-section}

La ilusi√≥n de la continuidad

---

## Desaf√≠o

¬øQu√© contiene la siguiente cadena de bits?

[00000000]{style="font-size: 80px; margin: 0px; color: white"}
[00101010]{style="font-size: 80px; margin: 0px;"}

---

[00000000]{style="font-size: 80px; margin: 0px; color: white"}
[00101010]{style="font-size: 80px; margin: 0px;"}

[Podr√≠a ser el n√∫mero 42 escrito en binario...]{.fragment}

---

[00000000]{style="font-size: 80px; margin: 0px; color: white"}
[00101010]{style="font-size: 80px; margin: 0px;"}

[Podr√≠a ser el car√°cter `*` en la convenci√≥n ascii...]{.fragment}

---

[00000000]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00101010]{style="font-size: 80px; margin: 0px;"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}

[Podr√≠a ser parte de un n√∫mero decimal, 0.4523 o $\pi$...]{.fragment}


---

[10000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00100001]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[01000111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00001000]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00101010]{style="font-size: 80px; margin: 0px;"}
[01000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[11111111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[11001101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[01000111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[00000101]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}
[01110111]{style="font-size: 80px; margin: 0px; color:#D3D3D3"}

[Podr√≠a ser parte de un archivo multimedia (video, imagen, audio, etc.)...]{.fragment}

---

En el computador **TODO** se representa con bits.

`bits` [`+` `contexto`]{.fragment fragment-index=1}  [`=`  `representaci√≥n`]{.fragment fragment-index=2}

::: {.callout-tip title="Aprendizaje" .fragment}
En el computador todo es discreto. No existe ni el infinito ni lo continuo.
:::

---

Ejemplo: derivada de una funci√≥n

![](code/derivada_05.png){fig-align="center"}

---
Ejemplo: derivada de una funci√≥n

![](code/derivada_07.png){fig-align="center"}

---
Ejemplo: derivada de una funci√≥n

![](code/derivada_09.png){fig-align="center"}

---
Ejemplo: derivada de una funci√≥n

![](code/derivada_11.png){fig-align="center"}

---
Ejemplo: derivada de una funci√≥n

![](code/derivada_21.png){fig-align="center"}

---
Ejemplo: derivada de una funci√≥n

![](code/derivada_31.png){fig-align="center"}

[No es necesario que sea exacto. Basta que sea suficientemente bueno...]{.fragment fragment-index=1}

---

No necesitamos la realidad, necesitamos una buena aproximaci√≥n.

![](images/celuloide.jpg){width=50% fig-align="center" .fragment}

[Una pel√≠cula de 24 FPS es suficiente para enga√±ar al ojo humano.]{.fragment}

[Necesitamos aproximaciones suficiente para enga√±ar a los sentidos.]{.fragment}

---

## ¬øQu√© aprendimos?

::: {.incremental}
* Ninguna representaci√≥n en el computador es perfecta.
* LLMs no son perfectos, pero ya son suficientemente √∫tiles.
:::

---

## Preguntas {.questions-slide}

---

## Parte 2 {.custom-section}

El computador parlanch√≠n

---

## ¬øC√≥mo representar una palabra?  

::: {.incremental}
* Representaci√≥n textual
* Representaci√≥n sem√°ntica
:::

---

## Representaci√≥n textual

Si solo queremos transcribir texto, basta con representar cada letra con una secuencia de bits, y almacenarla.

::: columns
::: {.column style="font-size: 24px;" .fragment .center}
ASCII

* 1 byte (8 bits): 128 car√°cteres posibles
* 0 (48) ... 9 (57)
* A (65) ... Z (90)
* a (97) ... z (122) 
* Problema: Faltan muchos car√°cteres: √ë√±√°√©√≠√≥√∫√º
:::
::: {.column style="font-size: 24px;" .fragment .center}
UTF-8

* 1 a 4 bytes (8 a 32 bits)
* Mantiene ASCII sin cambios
* Permite representar alfabetos latinos, griego, cir√≠lico, copto, armenio, hebreo, √°rabe, sir√≠aco, thaana, y n'ko, adem√°s de caracteres chinos, japoneses y coreanos. 
* Incluye emojis üòÅ, simbolos ‚úÖ y mil cosas m√°s üóø
:::
:::

---

## Representaci√≥n sem√°ntica

Sem√°ntica: relativo al **significado** de las palabras.

Si quieres que el computador pueda interpretar el sentido de cada palabra, es necesario almacenar cada palabra como un todo. No puede descomponerse en sus letras.

[Necesitamos una mejor REPRESENTACI√ìN.]{.fragment}

[La palabra se guarda entera o se descompone en sus partes representativas ([tokens]{style="color: red;"})]{.fragment}

---

## Analog√≠a

Lenguaje de se√±as: permite representar palabras textuales (letra a letra) y sem√°nticas (completas).

---

## Actividad 2.1

* **Actividad**: Sugerir textos para el tokenizador [http://localhost:8501/t](http://localhost:8501/t){target="_blank"} y [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer){target="_blank"}
* **Objetivo**: Evaluar distintos textos, en distintos idiomas.
  * Espa√±ol: [*La inform√°tica,‚Äã tambi√©n llamada computaci√≥n, es el √°rea de la ciencia que se encarga de estudiar la administraci√≥n de m√©todos, t√©cnicas y procesos con el fin de almacenar, procesar y transmitir informaci√≥n y datos en formato digital.*]{style="font-size: 24px;"}
  * Ingl√©s: [*Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes the study and experimentation of algorithmic processes, and the development of both hardware and software. Computing has scientific, engineering, mathematical, technological, and social aspects.*]{style="font-size: 24px;"}
* **Tiempo**: 5 minutos

[Nota: Definiciones extra√≠das de wikipedia.]{style="font-size: 18px;"}

---

## Desaf√≠o para LLMs

¬øPorqu√© un LLM no puede responder correctamente esta pregunta?

[How many r's are in strawberry?]{style="color: red;"}

![](images/strawberry.png){fig-align="center" width=500px}

[Pista: ¬øC√≥mo ve el LLM la palabra "strawberry"?]{.fragment fragment-index=1}

[Ahora que se ha vuelto un test conocido, las LLMs han aprendido la respuesta correcta. Pero no pueden "calcular" la respuesta.]{.fragment fragment-index=2}

---

## Aprendizajes 

::: {.incremental}
* Palabra != Token
* Cada token tiene un identificador √∫nico
* En ingl√©s,  100 tokens ~= 75 palabras. 
* Si tienen la misma representaci√≥n, tienen el mismo token (ejemplo: el papa y la papa)
:::

---

## ¬øLLM?

[LLM]{style="color: red;"} = Large Language Model = Grandes Modelos de Lenguaje

---

## Diagrama t√©cnico de un LLM

Diagrama de funcionamiento de un LLM que se filtr√≥ de OpenAI:

[**¬°¬°¬°No difundir!!!**]{style="color: red;" .fragment}

![](images/LLM_words.gif){fig-align="center" height=500px .fragment}

---

## Diagrama t√©cnico de un LLM

Ok, ok, en realidad es as√≠:

![](images/LLM.png){fig-align="center" width=700px}

---


## Actividad 2.2

* **Actividad**: Ir a [http://localhost:8501/nt](http://localhost:8501/nt){target="_blank"} y [https://huggingface.co/spaces/alonsosilva/NextTokenPrediction](https://huggingface.co/spaces/alonsosilva/NextTokenPrediction){target="_blank"}
* **Objetivo**: Observar la lista de token que se muestran como posible continuaci√≥n del texto.
* **Tiempo**: 5 minutos

---

## Aprendizajes 

::: {.incremental}
* El LLM no reflexiona sobre la pr√≥xima palabra: sugiere los tokens estad√≠sticamente m√°s probables.
* Se predice token a token: es secuencial.
* El LLM no tiene memoria. Siempre empieza a predecir desde el mismo estado inicial (como multiplicar matrices).
* El texto termina cuando se ha alcanzando la cantidad m√°xima de token posibles a procesar, o se ha generado un token de fin de texto <|endoftext|>.
:::

::: {.notes}
Comparar con una multiplicaci√≥n de matrices. La matriz no recuerda que ya hizo multiplicaciones antes.
:::

---

## El negocio de los LLMs 

::: {.incremental}
* Los LLMs hoy en d√≠a tienen cientos de miles de millones de par√°metros: 100,000,000,000.
* Cada par√°metro se determina en un proceso de entrenamiento basado en enormes conjuntos de texto (esencialmente TODO el internet y libros escritos).
* Entrenar una LLM requiere muchas horas de uso de tarjetas gr√°ficas (GPUs).
* Un LLM open source puede descargarse (son bits), y ejecutarse localmente - si tu hardware lo permite.
:::

---

## El negocio de los LLMs

No existe solo chatGPT (OpenAI): todos quieren un pedazo de la torta:

::: columns
::: {.column width="50%" .incremental .center}
**De pago:**

* GPT-4 (OpenAI)
* Gemini (Google)
* Claude (Anthropic)
* ...
:::
::: {.column width="50%" .incremental .center}
**C√≥digo abierto:**

* Llama (Meta)
* Qwen (Baidu)
* DeepSeek (China)
* ...
:::
:::

[Y est√°n quienes no desarrollan pero entregan LLM como un servicio: Amazon (Bedrock), OpenRouter, etc.]{.fragment}

---

## El negocio de los LLMs

![](images/ben_evans1.jpeg){fig-align="center" width=100%}

[SOTA: State of the Art]{style="font-size: 12px;"}

---

## El negocio de los LLMs

![](images/ben_evans2.jpeg){fig-align="center" width=100%}

---

## Aprendizajes 

::: {.incremental}
* LLMs han sido entrenados con millones de documentos: tienen el conocimiento potencial de todo el internet.
* LLMs trabajan con tokens, no con palabras.
* LLMs generan texto un token a la vez.
* Mientras m√°s grande es la frase proporcionada, m√°s coherente es la respuesta... por eso la importancia de un buen prompt.
* LLMs son una commodity
:::

---

## Preguntas {.questions-slide}

---

## Parte 3 {.custom-section}

¬°Hazlo tu mismo!

---

## ¬øC√≥mo podemos emular chatGPT?

¬øC√≥mo acceder a un LLM?

::: columns
::: {.column width="50%" .center}
**Ejecutar localmente LLM:** 

* Configuraci√≥n compleja
* Hardware costoso
* Hacer todos los pasos del proceso
:::
::: {.column width="50%"}
![](images/LLM_flow.png){fig-align="center"}
:::
:::

---

## ¬øC√≥mo podemos emular chatGPT?

¬øC√≥mo acceder a un LLM?

::: columns
::: {.column width="50%" .center}
![](images/LLM_API.png){fig-align="center"}
:::
::: {.column width="50%"}
**Consumir una API de LLM:**  

* Simple: entrega un texto, recibe un texto.
* Pagar lo que consumes
* M√∫ltiples proveedores y alternativas 
:::
:::

---

## Actividad 3.1

* **Actividad**: Ir a [https://cittripio.streamlit.app/v1](https://cittripio.streamlit.app/v1){target="_blank"}
* **Objetivo**: Lograr que el bot responda "con personalidad"
* **Tiempo**: 5 minutos

---

## Aprendizajes 

::: {.incremental}
* El LLM responde en funci√≥n del prompt.
* El prompt puede pedir cualquier cosa.
* Prompts cortos entregan resultados muy variables.
:::

---

## Actividad 3.2

* **Actividad**: Ir a [https://cittripio.streamlit.app/vdos](https://cittripio.streamlit.app/vdos){target="_blank"}
* **Objetivo**: 
  * O1: Hacer 2 preguntas relacionadas.
  * O2: Cambiar la personalidad del bot.
* **Tiempo**: 5 minutos

--- 

## Aprendizajes 

::: {.incremental}
* Separar en contexto y pregunta permite imponer una "personalidad" o ciertas caracter√≠sticas.
* El LLM no tiene memoria: solo predice el pr√≥ximo token en funci√≥n del texto proporcionado.
* El LLM genera texto "token a token", proporcionando la ilusi√≥n de una conversaci√≥n.
:::

--- 

## ¬øPorqu√© chatGPT si tiene memoria? {.fragment}

¬øC√≥mo solucionar√≠an ustedes este problema?

::: {.fragment .incremental}
Pas√©mosle la historia de la conversaci√≥n en cada prompt.

- Opci√≥n 1: Pasarle todo el texto.
- Opci√≥n 2: Pasarle un resumen de la conversaci√≥n.
:::

---

##  ¬øTemperatura?

¬øQu√© es la temperatura en un LLM?

::: {.incremental}
* Es un par√°metro que controla que tan aleatoria es la elecci√≥n del siguiente token.
  * Temperatura = 0: Muy determinista.
  * Temperatura = 1: Muy aleatorio.
:::

---

## Actividad 3.3

* **Actividad**: Ir a [https://cittripio.streamlit.app/vf](https://cittripio.streamlit.app/vf){target="_blank"}
* **Objetivo**: 
  * O1: Lograr que cittripio le responda a Luke Skywalker que es su padre.
  * O2: Cambiar la personalidad de cittripio por cualquier otro personaje (no necesariamente de Star Wars).
* **Tiempo**: 5 minutos

---

## Aprendizajes 

::: {.incremental}
* El LLM necesita tener como input todo el contexto e historia en el prompt.
* Las APIs agregan muchas opciones para simplificar y manejar todo esto convenientemente.
:::

---

## LLMs: Estado actual de la tecnolog√≠a

::: columns
::: {.column width="50%"}
LLMs en estado natural:

[ ![](images/potro_salvaje.jpg){fig-align="center" width=300px} ]{.fragment fragment-index=1}
:::
::: {.column width="50%"}
LLMs para uso en producci√≥n:

[ ![](images/potro_domado.jpg){fig-align="center" width=300px} ]{.fragment fragment-index=2}
:::
:::

---

## Gold Standard

Marco de evaluaci√≥n para la mejora continua de un "algoritmo" cuando:

- Existe una cantidad infinita de posibles preguntas a realizar
- Existe una cantidad infinita de verbalizaciones posibles para respuestas correctas

¬øC√≥mo saber si un LLM es "bueno"? ¬øC√≥mo saber si una nueva versi√≥n de LLM es "mejor" que la anterior?
---

## Preguntas del Gold Standard

- ["Gold Standard"]{style="color: #FFD700;"}: conjunto de 100 preguntas m√°s frecuentes del sistema, y una r√∫brica de evaluaci√≥n para cada una.
- Se eval√∫a como responde el sistema para cada una de las preguntas, y se corrije hasta que el sistema obtenga el m√°ximo puntaje posible.

---

## ¬øC√≥mo actualizar las preguntas del Gold Standard?

::: columns
::: {.column width="60%" style="font-size: 30px;"}
- ["Bronze Standard"]{style="color: #CD7F32;"}: Almacenar TODAS las preguntas, respuestas y retroalimentaci√≥n del sistema.
- ["Silver Standard"]{style="color: #C0C0C0;"}: Agrupar las preguntas por categor√≠as, calcular autom√°ticamente las preguntas m√°s frecuentes en un cierto periodo de tiempo (semana/mes) recopiladas en el "Bronze Standard".
- ["Gold Standard"]{style="color: #FFD700;"}: Manualmente actualizar la lista de preguntas y la r√∫brica, en base a las tendencias descubiertas en el "Silver Standard"

:::
::: {.column width="35%"}
![](images/piramide.jpg){fig-align="center" width=100%}
:::
:::

---

## Conclusi√≥n {.custom-slide}

::: {.incremental}
* LLMs no son magia: es tecnolog√≠a.
* Cualquiera puede comenzar a crear soluciones con LLMs.
* Conocer como funcionan LLMs permite usarlos mejor.
* Existen muchos recursos gratuitos para aprender y jugar.
:::

---

## { .custom-title }
::: {style="position: absolute; left: 700px; top: 550px; width:2000px; background-color: #ffffff; padding: 10px; border-radius: 5px;"}
[¬°Haz tu propio ChatGPT!]{style="font-size: 30px; font-weight: bold; margin: 0px"} <br>
:::
